{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Homework \n",
    "***\n",
    "**Name**: $<$Xu Han$>$ \n",
    "\n",
    "**Kaggle Username**: $<$xuhan$>$\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 23rd**. Additionally, you must make at least one submission to the **Kaggle** competition before it closes at **4:59pm on Friday February 23rd**. Submit only this Jupyter notebook to Moodle. Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "When people are discussing popular media, there’s a concept of spoilers. That is, critical information about the plot of a TV show, book, or movie that “ruins” the experience for people who haven’t read / seen it yet.\n",
    "\n",
    "The goal of this assignment is to do text classification on forum posts from the website [tvtropes.org](http://tvtropes.org/), to predict whether a post is a spoiler or not. We'll be using the logistic regression classifier provided by sklearn.\n",
    "\n",
    "Unlike previous assignments, the code provided with this assignment has all of the functionality required. Your job is to make the functionality better by improving the features the code uses for text classification.\n",
    "\n",
    "**NOTE**: Because the goal of this assignment is feature engineering, not classification algorithms, you may not change the underlying algorithm or it's parameters\n",
    "\n",
    "This assignment is structured in a way that approximates how classification works in the real world: Features are typically underspecified (or not specified at all). You, the data digger, have to articulate the features you need. You then compete against others to provide useful predictions.\n",
    "\n",
    "It may seem straightforward, but do not start this at the last minute. There are often many things that go wrong in testing out features, and you'll want to make sure your features work well once you've found them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle In-Class Competition \n",
    "***\n",
    "\n",
    "In addition to turning in this notebook on Moodle, you'll also need to submit your predictions on Kaggle, an online tournament site for machine learning competitions. The competition page can be found here:  \n",
    "\n",
    "[https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018](https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018)\n",
    "\n",
    "Additionally, a private invite link for the competition has been posted to Piazza. \n",
    "\n",
    "The starter code below has a `model_predict` method which produces a two column CSV file that is correctly formatted for Kaggle (predictions.csv). It should have the example Id as the first column and the prediction (`True` or `False`) as the second column. If you change this format your submissions will be scored as zero accuracy on Kaggle. \n",
    "\n",
    "**Note**: You may only submit **THREE** predictions to Kaggle per day.  Instead of using the public leaderboard as your sole evaluation processes, it is highly recommended that you perform local evaluation using a validation set or cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 1: Feature Engineering \n",
    "***\n",
    "\n",
    "The `FeatEngr` class is where the magic happens.  In it's current form it will read in the training data and vectorize it using simple Bag-of-Words.  It then trains a model and makes predictions.  \n",
    "\n",
    "25 points of your grade will be generated from your performance on the the classification competition on Kaggle. The performance will be evaluated on accuracy on the held-out test set. Half of the test set is used to evaluate accuracy on the public leaderboard.  The other half of the test set is used to evaluate accuracy on the private leaderboard (which you will not be able to see until the close of the competition). \n",
    "\n",
    "You should be able to significantly improve on the baseline system (i.e. the predictions made by the starter code we've provided) as reported by the Kaggle system.  Additionally, the top **THREE** students from the **PRIVATE** leaderboard at the end of the contest will receive 5 extra credit points towards their Problem 1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the length of the sentence    \n",
    "class LengthTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "                 \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples), 1))\n",
    "        \n",
    "        # Loop over examples and count length\n",
    "        for ii, x in enumerate(examples):\n",
    "            X[ii, :] = np.array([len(x)])\n",
    "            self.feature_names.append('length'+str(X[ii,:]))\n",
    "        return csr_matrix(X)\n",
    " \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "    \n",
    "    \n",
    "#LDA model    \n",
    "\n",
    "class LDATransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        \n",
    "        # create English stop words list\n",
    "        en_stop = set(stopwords.words('english'))\n",
    "        \n",
    "        # Create p_stemmer of class PorterStemmer\n",
    "        p_stemmer = PorterStemmer()\n",
    "        \n",
    "        # list for tokenized documents in loop\n",
    "        texts = []\n",
    "        \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples), 1))\n",
    "        \n",
    "        # Loop over examples and count letters \n",
    "        for ii, x in enumerate(examples):\n",
    "            # clean and tokenize document string\n",
    "            raw = x.lower()\n",
    "            tokens = tokenizer.tokenize(raw)\n",
    "            \n",
    "            # remove stop words from tokens\n",
    "            stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "            \n",
    "            # stem tokens\n",
    "            stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "            \n",
    "            # add tokens to list\n",
    "            texts.append(stemmed_tokens)\n",
    "            \n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        self.ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary)\n",
    "        \n",
    "        for ii, x in enumerate(examples):\n",
    "            # clean and tokenize document string\n",
    "            raw = x.lower()\n",
    "            tokens = tokenizer.tokenize(raw)\n",
    "            \n",
    "            # remove stop words from tokens\n",
    "            stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "            \n",
    "            # stem tokens\n",
    "            stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "            \n",
    "\n",
    "            bow = self.ldamodel.id2word.doc2bow(stemmed_tokens)\n",
    "            doc_topics, word_topics, phi_values = self.ldamodel.get_document_topics(bow,per_word_topics=True)\n",
    "            \n",
    "        \n",
    "            X[ii,:] = np.array(doc_topics[0][1])\n",
    "            \n",
    "            self.feature_names.append('lda'+str(X[ii,:]))\n",
    "            #print(X[ii,:])\n",
    "            \n",
    "        #X = preprocessing.normalize(X, norm='l2')\n",
    "        return csr_matrix(X)\n",
    "    \n",
    "    def topics(self,x):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        \n",
    "        # create English stop words list\n",
    "        en_stop = set(stopwords.words('english'))\n",
    "        \n",
    "        # Create p_stemmer of class PorterStemmer\n",
    "        p_stemmer = PorterStemmer()\n",
    "        \n",
    "        # clean and tokenize document string\n",
    "        raw = x.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "            \n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "            \n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "            \n",
    "\n",
    "        bow = self.ldamodel.id2word.doc2bow(stemmed_tokens)\n",
    "        doc_topics, word_topics, phi_values = self.ldamodel.get_document_topics(bow,per_word_topics=True)\n",
    "        print(doc_topics)\n",
    "    \n",
    "    def show(self):\n",
    "        print(self.ldamodel.show_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatEngr:\n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #self.vectorizer = CountVectorizer(stop_words='english')  # \"bag-of-words\" Accuracy: 0.67 (+/- 0.04)\n",
    "        #self.vectorizer_t = TfidfVectorizer(analyzer='word',ngram_range=(1,2), lowercase=True, norm='l2',stop_words='english')   # \"bag-of-words-tfidf\" Accuracy: 0.682206 (+/- 0.011113)\n",
    "        \n",
    "        #extract features from different columns\n",
    "        self.vectorizer = FeatureUnion( \n",
    "        [       \n",
    "                ('bag of words', \n",
    "                  Pipeline([('extract_field', FunctionTransformer(lambda x: x[0], validate = False)),\n",
    "                            ('tfid', TfidfVectorizer(analyzer='word',ngram_range=(1,2), lowercase=True, norm='l2',stop_words='english'))])),              \n",
    "                #('type of trope', \n",
    "                #  Pipeline([('extract_field', FunctionTransformer(lambda x: x[1], validate = False)),\n",
    "                #            ('trope', TfidfVectorizer())])),\n",
    "                ('length of sentence',\n",
    "                 Pipeline([('extract_field', FunctionTransformer(lambda x:  x[0], validate = False)), \n",
    "                            ('length', LengthTransformer())])),  \n",
    "                ('LDA model',\n",
    "                  Pipeline([('extract_field', FunctionTransformer(lambda x:  x[0], validate = False)), \n",
    "                            ('lda', LDATransformer())])),    \n",
    "        ])\n",
    "    \n",
    "    def build_train_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in training text features and do further feature engineering \n",
    "        Most of the work in this homework will go here, or in similar functions  \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        return self.vectorizer.fit_transform(examples)\n",
    "\n",
    "    def get_test_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in test text features and transform the same way as train features \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        return self.vectorizer.transform(examples)\n",
    "\n",
    "\n",
    "                \n",
    "    def train_model(self, random_state=1234):\n",
    "        \"\"\"\n",
    "        Method to read in training data from file, and \n",
    "        train Logistic Regression classifier. \n",
    "        \n",
    "        :param random_state: seed for random number generator \n",
    "        \"\"\"\n",
    "        \n",
    "         \n",
    "        \n",
    "        # load data \n",
    "        dfTrain = pd.read_csv(\"/home/yichen/Dropbox/CSCI 5922/Alexa-testing-tool/crawlerdata/train.csv\")\n",
    "        # get training features and labels \n",
    "        self.X_train = self.build_train_features([list(dfTrain[\"response\"])])\n",
    "        self.y_train = np.array(dfTrain[\"label\"], dtype=int)\n",
    "        \n",
    "        # train logistic regression model.  !!You MAY NOT CHANGE THIS!! \n",
    "        self.logreg = LogisticRegression(random_state=random_state)\n",
    "        self.logreg.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        scores = cross_val_score(self.logreg, self.X_train, self.y_train, cv =10)\n",
    "        print(\"train CV: %0.6f (+/- %0.6f)\" % (scores.mean(), scores.std() * 2))\n",
    "        \n",
    "            \n",
    "            \n",
    "    def model_predict(self):\n",
    "        \"\"\"\n",
    "        Method to read in test data from file, make predictions\n",
    "        using trained model, and dump results to file \n",
    "        \"\"\"\n",
    "        \n",
    "        # read in test data \n",
    "        dfTest  = pd.read_csv(\"/home/yichen/Dropbox/CSCI 5922/Alexa-testing-tool/crawlerdata/test.csv\")\n",
    "        \n",
    "        # featurize test data \n",
    "        self.X_test = self.get_test_features([list(dfTest[\"response\"])])\n",
    "        \n",
    "        # make predictions on test data \n",
    "        pred = self.logreg.predict(self.X_test)\n",
    "        \n",
    "        # dump predictions to file for submission to Kaggle  \n",
    "        self.y_test = dfTest['label'].tolist()\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        print(accuracy_score(pred,self.y_test))\n",
    "        \n",
    "        print(\"confusion matrix:\")\n",
    "        print(confusion_matrix(pred,self.y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train CV: 0.615071 (+/- 0.038506)\n",
      "0.6304849884526559\n",
      "confusion matrix:\n",
      "[[ 50   5   0   0   5   0   1   6   0   0   2   0   0   0   0]\n",
      " [  3  55   1   1   5   3   5   5   1   1   0   0   1   2   0]\n",
      " [  0   0  15   0   0   0   0   0   0   0   0   0   0   0   1]\n",
      " [  6  14   5 173  10  10   9  22   2   7   8   2   6   6   6]\n",
      " [  2   1   4   3  56   1   4   3   2   0   1   1   5   0   3]\n",
      " [  0   0   0   0   1  22   1   0   0   0   0   0   0   0   0]\n",
      " [  1   1   0   0   0   1  14   1   0   0   4   0   0   0   0]\n",
      " [  8  11   4  17  10  12  11  84   0   7   4   3   2   1   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2   1   0   0   0   1   0   0   0   4   0   0   0   0   0]\n",
      " [  1   0   0   3   0   0   0   0   0   5  11   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   0   0  12   0]\n",
      " [  2   1   0   1   1   0   3   1   0   0   1   0   2   0  49]]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the FeatEngr clas \n",
    "feat = FeatEngr()\n",
    "\n",
    "# Train your Logistic Regression classifier \n",
    "feat.train_model(random_state=1230)\n",
    "\n",
    "# Make prediction on test data and produce Kaggle submission file \n",
    "feat.model_predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
